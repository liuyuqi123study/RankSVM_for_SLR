[train] 
epoch = 5
batch_size = 4

reader_num = 1

optimizer = torch.optim.AdamW
learning_rate = 5e-6
weight_decay = 0
step_size = 1
lr_multiplier = 1

PLM_vocab =  hfl/chinese-roberta-wwm-ext
PLM_path = thunlp/Lawformer
query_len = 509
cand_len = 3072

[eval] #eval parameters
batch_size = 2

reader_num = 1

[data] #data parameters
train_dataset_type = pairwise
train_formatter_type = lawformer

valid_dataset_type = pairwise
valid_formatter_type = lawformer

test_dataset_type = pairwise
test_formatter_type = lawformer

query_path = /content/drive/MyDrive/CAIL2024/LEVEN-main-2/Downstreams/SCR/SCR-Preprocess/output_data-supervised/query/train_query.json
cand_path = /content/input/candidates/content/input/candidates_subfolder
label_path =/content/drive/MyDrive/CAIL2024/LEVEN-main-2/Downstreams/SCR/SCR-Experiment_me/input_data/label/label_order.json
test_file =/content/drive/MyDrive/CAIL2024/LEVEN-main-2/Downstreams/SCR/SCR-Preprocess/output_data-supervised/query/test_query.json                

#result_path = ./result/EDBERT/test
result_path=./result/lawformer/test
[model] 
#model_name = pairwise
model_name=lawformer
#use_event = True  
use_event=False                  

[output] #output parameters
output_time = 1
test_time = 1

model_path = ./output/model
model_name = Lawformer

tensorboard_path = ./output/tensorboard

output_function = out1
tqdm_ncols = 150
