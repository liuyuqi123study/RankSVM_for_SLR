[train] 
epoch = 5
batch_size = 2

reader_num = 1

optimizer = torch.optim.AdamW
learning_rate = 2.5e-6
weight_decay = 0
step_size = 1
lr_multiplier = 1

PLM_vocab =  hfl/chinese-roberta-wwm-ext
PLM_path = thunlp/Lawformer
query_len = 509
cand_len = 3072

[eval] #eval parameters
batch_size = 2

reader_num = 1

[data] #data parameters
train_dataset_type = pairwise
train_formatter_type = lawformer

valid_dataset_type = pairwise
valid_formatter_type = lawformer

test_dataset_type = pairwise
test_formatter_type = lawformer

query_path = ./input_data/query/query_5fold
cand_path = ./input_data/candidates
label_path = ./input_data/label/golden_labels.json
test_file = 4                        

#result_path = ./result/EDBERT/test
result_path=./result/lawformer/test
[model] 
#model_name = pairwise
model_name=lawformer
#use_event = True  
use_event=False                  

[output] #output parameters
output_time = 1
test_time = 1

model_path = ./output/model
model_name = Lawformer_fold1

tensorboard_path = ./output/tensorboard

output_function = out1
tqdm_ncols = 150
