[train] 
epoch = 5
batch_size = 16

reader_num = 1

optimizer = torch.optim.AdamW
learning_rate = 1e-5
weight_decay = 0
step_size = 1
lr_multiplier = 1

PLM_vocab = hfl/chinese-roberta-wwm-ext
PLM_path = thunlp/lawformer
query_len = 509
cand_len = 3072

[eval] #eval parameters
batch_size = 16

reader_num = 1

[data] #data parameters
train_dataset_type = pairwise
train_formatter_type = lawformer

valid_dataset_type = pairwise
valid_formatter_type = lawformer

test_dataset_type = pairwise
test_formatter_type = lawformer

query_path = /content/drive/MyDrive/LEVEN-main/Downstreams/SCR/SCR-Experiment/input_data/query/query_5fold
cand_path = /content/input/candidates/candidates
label_path = /content/drive/MyDrive/LEVEN-main/Downstreams/SCR/SCR-Experiment/input_data/label/golden_labels.json
test_file = 0                        

#result_path = ./result/EDBERT/test
result_path=./result/lawformer/test
[model] 
#model_name = pairwise
model_name=lawformer
#use_event = True  
use_event=False                  

[output] #output parameters
output_time = 1
test_time = 1

model_path = ./output/model
model_name = Bert3

tensorboard_path = ./output/tensorboard

output_function = out1
tqdm_ncols = 150
